---
layout: post
title: 《从头开始构建大语言模型》03
date: 2024-07-18
author: 曾伟
tags: [论文阅读]
comments: true
toc: true # 目录
# pinned: true 是否置顶
---

简介：这是《从头开始构建大语言模型》的第三章。

## 3 编程实现自注意力机制
> 本章涵盖：
> * 探讨在神经网络中使用注意机制的原因
> * 引入基本的自注意力框架，并逐步发展为增强的自注意力机制
> * 实现一个因果注意力模块，允许LLMs每次生成一个token
> * 用dropout屏蔽随机选择的注意力权重，以减少过拟合
> * 将多个因果注意模块堆叠成一个多头注意模块

在前一章中，您学习了如何为训练LLMs准备输入文本。这涉及到将文本分割成单独的词和子词tokens，这些tokens可以编码为LLM的向量表示，即所谓的嵌入向量。

在本章中，我们将研究LLM架构本身的一个组成部分——注意机制，如图3.1所示。

![图 3.1](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image001.png)
图 3.1 对LLM进行编码、在一般文本数据集上对LLM进行预训练以及在标记数据集上对其进行微调的三个主要阶段的模型流程图。这一章的重点是自注意力机制，这是LLM架构的一个组成部分。

注意机制是一个复杂的主题，这就是为什么我们要用一整章来讨论它。我们将在很大程度上孤立地看待这些注意机制，并在机制层面上关注它们。在下一章中，我们将对LLM中自注意力机制的其余部分进行编程，以了解它的作用并创建一个模型来生成文本。

在本章的过程中，我们将实现注意机制的四种不同变体，如图3.2所示。

![图 3.2](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image003.png)
图 3.2 该图描述了我们将在本章中编码的不同注意机制，在添加可训练权重之前，从简化版本的自注意开始。因果注意机制为自注意添加了一个掩码，允许LLM一次生成一个单词。最后，多头注意将注意力机制组织成多个头部，允许模型并行捕获输入数据的各个方面。

图3.2所示的这些不同的注意力变体相互构建，目标是在本章结束时实现一个紧凑而有效的多头注意力实现，然后我们可以将其插入到LLM架构中，我们将在下一章中编程实现。

### 3.1 长序列建模的问题
在我们深入研究本章后面LLMs的核心——自注意机制之前，在LLMs之前没有注意机制的架构会有什么问题?假设我们想开发一个语言翻译模型，将文本从一种语言翻译成另一种语言。如图3.3所示，由于源语和目的语的语法结构不同，我们不能简单地逐字翻译文本。

![图 3.3](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image005.png)
图 3.3 在将文本从一种语言翻译成另一种语言时，例如将德语翻译成英语，不可能只是逐字翻译。相反，翻译过程需要上下文理解和语法对齐。

为了解决我们无法逐字翻译文本的问题，通常使用具有两个子模块的深度神经网络，即所谓的编码器和解码器。编码器的工作是首先读取并处理整个文本，然后解码器生成翻译后的文本。

当我们在第1章(第1.4节，为不同的任务使用LLMs)中介绍变压器架构时，我们已经简要地讨论了编码器-解码器网络。在变压器出现之前，递归神经网络(RNN)是语言翻译中最流行的编码器-解码器架构。

RNN是一种神经网络，其中前一步的输出作为当前步骤的输入，使它们非常适合于文本等顺序数据。如果你不熟悉RNN，不要担心，你不需要知道RNN的详细工作原理来参与这个讨论;我们在这里更多地关注编码器-解码器设置的一般概念。

在编码器-解码器RNN中，输入文本被送入编码器，编码器依次处理文本。编码器在每一步更新其隐藏状态(隐藏层的内部值)，试图在最终隐藏状态下捕获输入句子的整个含义，如图3.4所示。然后，解码器利用这个最终的隐藏状态开始生成翻译后的句子，一次一个单词。它还在每一步更新其隐藏状态，该状态应该携带下一个单词预测所需的上下文。

![图 3.4](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image007.png)
图 3.4 在Transformer模型出现之前，编码器-解码器RNN是机器翻译的热门选择。编码器从源语言中获取一系列tokens作为输入，其中编码器的隐藏状态(中间神经网络层)编码整个输入序列的压缩表示。然后，解码器使用其当前的隐藏状态开始翻译，一个接一个的tokens。

我们不需要知道这些编码器-解码器RNN的内部工作原理，但这里的关键思想是编码器部分将整个输入文本处理为隐藏状态(存储单元)。然后，解码器接受这种隐藏状态来产生输出。你可以把这个隐藏状态想象成一个嵌入向量，这个概念我们在第二章讨论过。

编码器-解码器RNN的一个大问题和局限性是，在解码阶段，RNN不能直接访问编码器先前的隐藏状态。因此，它完全依赖于当前隐藏状态，它封闭了所有相关信息。这可能导致上下文的丢失，特别是在复杂的句子中，依赖关系可能跨越很长的距离。

对于不熟悉RNN的读者，没有必要理解或研究这种架构，因为我们将不会在本书中使用它。本节的要点是，编码器-解码器RNN有一个缺点，这激发了注意力机制的设计。


### 3.2 使用注意机制捕获数据依赖关系
在Transformer LLMs之前，如前所述，通常使用RNN来完成语言建模任务，如语言翻译。RNN可以很好地翻译短句，但对于较长的文本就不太适用了，因为它们不能直接访问输入中的前一个单词。

这种方法的一个主要缺点是，RNN必须在将其传递给解码器之前以单一隐藏状态记住整个编码输入，如前一节中的图3.4所示。

因此，研究人员在2014年开发了RNN的Bahdanau注意机制(以相应论文的第一作者命名)，该机制修改了编码器-解码器RNN，使解码器可以在每个解码步骤中选择性地访问输入序列的不同部分，如图3.5所示。

![图 3.5](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image009.png)
图 3.5 使用注意机制，网络的文本生成解码器部分可以选择性地访问所有输入tokens。这意味着对于生成给定的输出tokens，一些输入tokens比其他输入token更重要。重要性是由所谓的注意力权重决定的，我们将在后面计算。请注意，该图显示了注意力背后的一般思想，并没有描述Bahdanau机制的确切实现，这是本书范围之外的RNN方法。

有趣的是，仅仅三年后，研究人员发现构建用于自然语言处理的深度神经网络并不需要RNN架构，并提出了原始的transformer架构(在第1章中讨论)，该架构具有受Bahdanau注意机制启发的自注意机制。

自注意力是一种机制，它允许输入序列中的每个位置在计算序列表示时关注同一序列中的所有位置。自注意力机制是基于Transformer体系结构的当代LLMs(如GPT系列)的一个关键组成部分。

本章的重点是编码和理解GPT类模型中使用的这种自注意机制，如图3.6所示。在下一章中，我们将对LLM的其余部分进行编程。

![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image011.png)
图 3.6 自注意力是Transformer中的一种机制，通过允许序列中的每个位置与同一序列中所有其他位置相互作用并权衡其重要性，用于计算更有效的输入表示。在本章中，我们将从头开始编写这种自我注意机制，然后在下一章中编写类似GPT的LLM的其余部分。

### 3.3 使用自注意力关注输入的不同部分
现在，我们将深入研究自我注意力机制的内部工作原理，并学习如何从头开始编写它。自注意力是每个基于Transformer体系结构的LLM的基石。

值得注意的是，这个主题可能需要大量的关注和注意(没有双关语的意思)，但是一旦你掌握了它的基本原理，你就征服了本书中最难的一个方面，并在总体上实现了LLMs。

> 自注意力中的“自”
>
> 在自注意中，“自”指的是该机制通过关联同个输入序列中的不同位置来计算注意权重的能力。它评估和学习输入本身的各个部分之间的关系和依赖关系，例如句子中的单词或图像中的像素。这与传统的注意力机制形成了对比，传统的注意力机制关注的是两个不同序列元素之间的关系，比如在序列到序列模型中，注意力可能在输入序列和输出序列之间比较，如图3.5所示。

因为自我注意看起来很复杂，特别是当你第一次遇到它的时候，我们将在下一小节开始介绍一个简化版本的自我注意。之后，在3.4节中，我们将实现在LLMs中使用的具有可训练权值的自注意力机制。

#### 3.3.1 没有可训练权重的简单自注意机制
在本节中，我们实现了一种简化的自注意力变体，没有任何可训练的权重，如图3.7所示。本节的目的是在3.4节添加可训练权重之前，说明自我注意力中的几个关键概念。

![图 3.7](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image013.png)
图 3.7 自注意的目标是为每个输入元素计算一个上下文向量，它结合了来自所有其他输入元素的信息。在图中描述的示例中，我们计算上下文向量$z^{(2)}$。计算$z^{(2)}$的每个输入元素的重要性或贡献由注意力权重$α_{21}$到$α_{2T}$决定。在计算$z^{(2)}$时，注意力权重是根据输入元素$x^{(2)}$和所有其他输入来计算的。这些注意力权重的精确计算将在本节后面讨论

图3.7给出了一个输入序列，用$x$表示，由$T$个元素组成，用$x^{(1)}$到$x^{(T)}$表示。这个序列通常表示文本，比如一个句子，它已经被转换成tokens嵌入，如第2章所述。

例如，考虑这样一个输入文本:“Your journey starts with one step.”，在这种情况下，序列的每个元素，例如$x^{(1)}$，对应于表示特定标记(如“Your”)的$d$维嵌入向量。在图3.7中，这些输入向量显示为三维嵌入。

在自注意力中，我们的目标是计算输入序列中每个元素$x^{(i)}$的上下文向量$z^{(i)}$。上下文向量可以解释为一个丰富的嵌入向量。

为了说明这个概念，让我们把重点放在第二个输入元素的嵌入向量$x^{(2)}$(对应于标记“journey”)和相应的上下文向量$z^{(2)}$上，如图3.7底部所示。这个增强的上下文向量$z^{(2)}$是一个包含关于$x^{(2)}$和所有其他输入元素$x^{(1)}$到$x^{(T)}$的信息的嵌入。

在自注意力中，语境向量起着至关重要的作用。它们的目的是通过合并序列中所有其他元素的信息来创建输入序列(如句子)中每个元素的丰富表示，如图3.7所示。这在LLMs中是必不可少的，因为LLMs需要理解句子中单词之间的关系和相关性。稍后，我们将添加可训练的权重，帮助LLM学习构建这些上下文向量，以便它们生成与LLM相关的下一个token。

在本节中，我们实现了一个简化的自注意力机制来一步一步地计算这些权重和结果上下文向量。

考虑下面的输入句子，它已经被嵌入到第2章讨论的三维向量中。我们选择了一个小的嵌入尺寸，以确保它适合没有换行的页面:
```python
import torch
inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)
```

实现自我注意的第一步是计算中间值$ω$，称为注意力得分，如图3.8所示。

![图 3.8](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image015.png)
图 3.8 本节的总体目标是说明使用第二个输入序列$x^{(2)}$作为query来计算上下文向量$z^{(2)}$。该图显示了第一个中间步骤，通过点积计算query $x^{(2)}$与所有其他输入元素之间的注意力分数$ω$。(请注意，图中的数字在小数点后被截断为一位，以避免混乱。)

图3.8说明了我们如何计算query token和每个输入token之间的中间注意力分数。我们通过计算query $x^{(2)}$与每个其他输入token的点积来确定这些分数:
```python
query = inputs[1]
attn_scores_2 = torch.empty(inputs.shape[0])
for i, x_i in enumerate(inputs):
    attn_scores_2[i] = torch.dot(x_i, query)
print(attn_scores_2)
```

计算出的注意分数如下:
```python
tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])
```

> 理解点积
> 
> 点积本质上是一种简洁的计算方法，将两个向量的逐元素相乘，然后将乘积相加，我们可以这样演示:
> ```python
> res = 0.
> for idx, element in enumerate(inputs[0]):
>     res += inputs[0][idx] * query[idx]
> print(res)
> print(torch.dot(inputs[0], query))
> ```
> 输出证实了逐元素乘法的和与点积的结果相同:
> ```python
> tensor(0.9544)
> tensor(0.9544)
> ```
> 除了将点积运算视为一种将两个向量组合起来产生标量值的数学工具之外，点积还是一种相似性度量，因为它量化了两个向量对齐的程度:点积越高表示向量之间对齐或相似的程度越高。在自注意力机制的背景下，点积决定了序列中元素相互关注的程度:点积越高，两个元素之间的相似性和注意力得分越高。

在下一步中，如图3.9所示，我们将之前计算的每个注意力分数归一化。

![图 3.9](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image017.png)
图 3.9 对输入query $x^{(2)}$计算出注意分数$ω_{21} \sim ω_{2T}$后，通过对注意分数进行归一化，得到注意力权重$α_{21} \sim α_{2T}$。


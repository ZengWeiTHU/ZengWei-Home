---
layout: post
title: 《从头开始构建大语言模型》03
date: 2024-07-18
author: 曾伟
tags: [论文阅读]
comments: true
toc: true # 目录
# pinned: true 是否置顶
---

简介：这是《从头开始构建大语言模型》的第三章。

## 3 编程实现自注意力机制
> 本章涵盖：
> * 探讨在神经网络中使用注意机制的原因
> * 引入基本的自注意力框架，并逐步发展为增强的自注意力机制
> * 实现一个因果注意力模块，允许LLMs每次生成一个token
> * 用dropout屏蔽随机选择的注意力权重，以减少过拟合
> * 将多个因果注意模块堆叠成一个多头注意模块

在前一章中，您学习了如何为训练LLMs准备输入文本。这涉及到将文本分割成单独的词和子词tokens，这些tokens可以编码为LLM的向量表示，即所谓的嵌入向量。

在本章中，我们将研究LLM架构本身的一个组成部分——注意机制，如图3.1所示。

![图 3.1](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image001.png)
图 3.1 对LLM进行编码、在一般文本数据集上对LLM进行预训练以及在标记数据集上对其进行微调的三个主要阶段的模型流程图。这一章的重点是自注意力机制，这是LLM架构的一个组成部分。

注意机制是一个复杂的主题，这就是为什么我们要用一整章来讨论它。我们将在很大程度上孤立地看待这些注意机制，并在机制层面上关注它们。在下一章中，我们将对LLM中自注意力机制的其余部分进行编程，以了解它的作用并创建一个模型来生成文本。

在本章的过程中，我们将实现注意机制的四种不同变体，如图3.2所示。

![图 3.2](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image003.png)
图 3.2 该图描述了我们将在本章中编码的不同注意机制，在添加可训练权重之前，从简化版本的自注意开始。因果注意机制为自注意添加了一个掩码，允许LLM一次生成一个单词。最后，多头注意将注意力机制组织成多个头部，允许模型并行捕获输入数据的各个方面。

图3.2所示的这些不同的注意力变体相互构建，目标是在本章结束时实现一个紧凑而有效的多头注意力实现，然后我们可以将其插入到LLM架构中，我们将在下一章中编程实现。

### 3.1 长序列建模的问题
在我们深入研究本章后面LLMs的核心——自注意机制之前，在LLMs之前没有注意机制的架构会有什么问题?假设我们想开发一个语言翻译模型，将文本从一种语言翻译成另一种语言。如图3.3所示，由于源语和目的语的语法结构不同，我们不能简单地逐字翻译文本。

![图 3.3](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image005.png)
图 3.3 在将文本从一种语言翻译成另一种语言时，例如将德语翻译成英语，不可能只是逐字翻译。相反，翻译过程需要上下文理解和语法对齐。

为了解决我们无法逐字翻译文本的问题，通常使用具有两个子模块的深度神经网络，即所谓的编码器和解码器。编码器的工作是首先读取并处理整个文本，然后解码器生成翻译后的文本。

当我们在第1章(第1.4节，为不同的任务使用LLMs)中介绍变压器架构时，我们已经简要地讨论了编码器-解码器网络。在变压器出现之前，递归神经网络(RNN)是语言翻译中最流行的编码器-解码器架构。

RNN是一种神经网络，其中前一步的输出作为当前步骤的输入，使它们非常适合于文本等顺序数据。如果你不熟悉RNN，不要担心，你不需要知道RNN的详细工作原理来参与这个讨论;我们在这里更多地关注编码器-解码器设置的一般概念。

在编码器-解码器RNN中，输入文本被送入编码器，编码器依次处理文本。编码器在每一步更新其隐藏状态(隐藏层的内部值)，试图在最终隐藏状态下捕获输入句子的整个含义，如图3.4所示。然后，解码器利用这个最终的隐藏状态开始生成翻译后的句子，一次一个单词。它还在每一步更新其隐藏状态，该状态应该携带下一个单词预测所需的上下文。

![图 3.4](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch03__image007.png)
图 3.4 在Transformer模型出现之前，编码器-解码器RNN是机器翻译的热门选择。编码器从源语言中获取一系列tokens作为输入，其中编码器的隐藏状态(中间神经网络层)编码整个输入序列的压缩表示。然后，解码器使用其当前的隐藏状态开始翻译，一个接一个的tokens。

我们不需要知道这些编码器-解码器RNN的内部工作原理，但这里的关键思想是编码器部分将整个输入文本处理为隐藏状态(存储单元)。然后，解码器接受这种隐藏状态来产生输出。你可以把这个隐藏状态想象成一个嵌入向量，这个概念我们在第二章讨论过。

编码器-解码器RNN的一个大问题和局限性是，在解码阶段，RNN不能直接访问编码器先前的隐藏状态。因此，它完全依赖于当前隐藏状态，它封闭了所有相关信息。这可能导致上下文的丢失，特别是在复杂的句子中，依赖关系可能跨越很长的距离。

对于不熟悉RNN的读者，没有必要理解或研究这种架构，因为我们将不会在本书中使用它。本节的要点是，编码器-解码器RNN有一个缺点，这激发了注意力机制的设计。


### 3.2 使用注意机制捕获数据依赖关系
在Transformer LLMs之前，如前所述，通常使用RNN来完成语言建模任务，如语言翻译。RNN可以很好地翻译短句，但对于较长的文本就不太适用了，因为它们不能直接访问输入中的前一个单词。

这种方法的一个主要缺点是，RNN必须在将其传递给解码器之前以单一隐藏状态记住整个编码输入，如前一节中的图3.4所示。
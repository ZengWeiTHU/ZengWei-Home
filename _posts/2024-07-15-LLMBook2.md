---
layout: post
title: 《从头开始构建大语言模型》02
date: 2024-07-14
author: 曾伟
tags: [论文阅读]
comments: true
toc: true # 目录
# pinned: true 是否置顶
---

简介：紧接[上一章](https://zengweithu.github.io/ZengWei-Home/LLMBook1/)，这是《从头开始构建大语言模型》的第2章。紧急消息：原来在[MANNING上的Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)上分章阅读当我读到2.1的时候就不给我读了，提示我需要收费，好吧，非常sad。看来不得不掏出我自己找到的老版[PDF](https://github.com/ZengWeiTHU/eBook/blob/main/LLM/Build-a-Large-Language-Model-From-Scratch.pdf)了。另外，我还发现[SCRIBD](https://www.scribd.com/home)也只有前30天免费，果然这是一个处处要钱的世界。

## 2 使用文本数据
> 本章涵盖：
> * 为大型语言模型训练准备文本
> * 将文本拆分为单词和子单词tokens
> * tokenizing文本更高级的字节对编码
> * 使用滑动窗口方法的抽样训练样本
> * 将tokens转换为输送到大型语言模型的向量

在上一章中，我们介绍了大型语言模型（LLMs）的通用结构，并了解到它们是在大量文本数据上进行预先训练的。具体来说，我们的重点是在基于Transformer架构的LLM中的解码器，该架构是 ChatGPT 和其他流行的类似 LLMsGPT 的模型的基础。

在预训练阶段，LLMs一次处理一个单词文本。使用下一个单词预测任务对数百万到数十亿个参数进行LLMs训练，可以产生具有令人印象深刻功能的模型。然后，可以进一步微调这些模型，以遵执行通用命令或特定的目标任务。但是，在接下来的章节中实现和训练LLMs之前，我们需要准备训练数据集，这是本章的重点，如图 2.1 所示。

![图 2.1](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image001.png)
图 2.1 编程实现一个LLM有三个阶段，其中包括LLM在通用文本数据集上进行预训练，并在标记数据集上对其进行微调。本章将解释和编程实现数据的准备和采样流程，该流程提供用于LLM预训练的文本数据。

在本章中，您将学习如何准备用于训练LLMs的输入文本。这涉及将文本拆分为单独的单词和子单词tokens，然后可以将其编码为LLM需要的向量表示。您还将了解高级tokenization方案，例如字节对编码，该方案用于GPT 等流行LLMs。最后，我们将实现采样和数据加载策略，以生成后续章节中训练LLMs所需的输入-输出对。

### 2.1 理解词嵌入
深度神经网络模型（包括 LLMs）无法直接处理原始文本。由于文本是离散的，因此它与用于实现和训练神经网络的数学运算不兼容。因此，我们需要一种方法来将单词表示为连续值向量。（不熟悉计算中的向量和张量的读者可以在附录 A 的 A2.2 节“理解张量”中了解更多信息。）

将数据转换为向量格式这一概念通常称为嵌入（embedding）。使用特定的神经网络层或其他预训练的神经网络模型，我们可以嵌入不同的数据类型，例如视频、音频和文本，如图 2.2 所示。

![图 2.2](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image003.png)
图 2.2 深度学习模型无法以原始形式处理视频、音频和文本等数据格式。因此，我们使用嵌入模型将这些原始数据转换为深度学习架构可以轻松理解和处理的稠密向量表示。具体来说，该图说明了将原始数据转换为三维数值向量的过程。

如图2.2所示，我们可以通过嵌入模型来处理各种不同的数据格式。但是，需要注意的是，不同的数据格式需要不同的嵌入模型。例如，为文本设计的嵌入模型不适合嵌入音频或视频数据。

嵌入的核心是从离散对象(如单词、图像甚至整个文档)到连续向量空间中的点的映射——嵌入的主要目的是将非数字数据转换为神经网络可以处理的数据格式。

虽然词嵌入是最常见的文本嵌入形式，但也有句子、段落或整个文档的嵌入。句子或段落嵌入是检索增强式生成（retrievalaugmented generation）的常用选择。检索式增强生成将生成(如生成文本)和检索(如搜索外部知识库)结合起来，在生成文本时提取相关信息，这是一种超出本书范围的技术。由于我们的目标是训练类似GPT的LLMs，它一次生成一个词，因此本章主要关注词嵌入。

目前已经开发了几种算法和框架来生成词嵌入，一个较早且最流行的例子是Word2Vec方法。Word2Vec训练神经网络架构，通过预测给定目标词的词的上下文来生成词嵌入，反之亦然。Word2Vec背后的主要思想是：出现在相似上下文中的单词往往具有相似的含义。因此，为了可视化的目的，当投影到二维词嵌入中时，可以看到相似的术语单词聚集在一起，如图2.3所示。

![图 2.3 ](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image005.png)
图 2.3 如果词嵌入是二维的，我们可以将它们绘制在二维散点图中，以便可视化，如下所示。当使用词嵌入技术（如Word2Vec）时，将相似概念相对应的词在嵌入空间中经常彼此靠近。例如，与国家和城市相比，不同类型的鸟类在嵌入空间中看起来彼此更接近。

单词嵌入可以有不同的维度，从1到数千。如图2.3所示，为了可视化目的，我们可以选择二维词嵌入。更高的维度可能捕获更细微的关系，但代价是计算效率。

虽然我们可以使用预训练模型(如Word2Vec)为机器学习模型生成嵌入，但LLMs通常会生成自己的嵌入，这些嵌入是输入层的一部分，并在训练期间更新。作为LLMs训练的一部分而不是使用Word2Vec来进行词嵌入的优点是，该方式针对当前的特定任务的数据进行了优化。我们将在本章后面实现这样的嵌入层。此外，LLMs还可以创建上下文相关的输出词嵌入，我们将在第3章中讨论。

不幸的是，高维嵌入给可视化带来了挑战，因为我们的感官知觉和常见的图形表示本质上局限于三维或更少的维度，这就是为什么图2.3在二维散点图中显示二维嵌入。然而，当使用LLMs时，我们通常使用比图2.3所示的维度高得多的词嵌入。对于GPT-2和GPT-3，嵌入大小(通常称为模型隐藏状态的维数)根据特定的模型变量和大小而变化。这是性能和效率之间的权衡。最小的GPT-2模型(117M和125M参数)使用768维的嵌入尺寸作为具体的例子。最大的GPT-3模型(175B参数)则使用12288个维度的嵌入大小。

本章接下来的章节将介绍准备LLM使用的嵌入所需的步骤，包括将文本分解为单词，将单词转换为tokens，并将tokens转换为嵌入向量。

### 2.2 Tokenizing文本
本节介绍如何将输入文本拆分为单个tokens，这是为LLM创建嵌入所需的必要预处理步骤。这些符号可以是单独的单词或特殊字符，包括标点符号，如图2.4所示。

![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image007.png)
图2.4 本节中涉及在LLM上下文中的进行文本处理的可视化流程图。这里，我们将输入文本拆分为单独的tokens，这些tokens可以是单词，也可以是特殊字符，如标点符号。在接下来的部分中，我们将把文本转换为tokens IDs并创建tokens嵌入。

我们将用于LLM训练tokens的文本是伊迪丝·沃顿(Edith Wharton)的一篇短篇小说《判决》(The Verdict)，它是公开的，因此被允许用于LLMs训练任务。该文本可在Wikisource：https://en.wikisource.org/wiki/The_Verdict 上获得，您可以将其复制并粘贴到文本文件中，我将其复制到文本文件"the-verdict.txt"中，以便使用Python的标准文件读取实用程序加载:

**<sub>清单2.1将一个短篇故事作为文本样本读入Python</sub>**
```python
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
print("Total number of character:", len(raw_text))
print(raw_text[:99])
```

或者，您可以在本书的GitHub存储库 https://github.com/rasbt/LLMs-fromscratch/tree/main/ch02/01_main-chapter-code 中找到“the-verdict.txt”文件。

为了便于说明，print命令打印该文件的前100个字符后的总字符数:
```python
Total number of character: 20479
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no 
```

我们的目标是将这20,479个字符的短篇故事tokenize为单个单词和特殊字符，然后我们可以在接下来的章节中将其转化为LLMs训练的次嵌入。

> 文本样本的大小
>
> 请注意，在训练LLMs时，通常要处理数百万篇文章和数十万本书(许多GB的文本)。但是，出于教育目的，使用较小的文本示例(例如一本书)来说明文本处理步骤背后的主要思想并使其能够在合理的时间内在消费者硬件上运行就足够了。

我们如何分割文本以获得tokens列表?为此，我们进行了一个小的尝试，并使用Python的正则表达式库re进行说明。(注意，你不需要学习或记忆任何正则表达式语法，因为我们将在本章后面会过渡到提前构建的tokenizer函数。)

使用一些简单的示例文本，我们可以使用re.split命令和以下语法来分割空白字符的文本:
```python
import re
text = "Hello, world. This, is a test."
result = re.split(r'(\s)', text)
print(result)
```

结果是一个由单个单词、空格和标点字符组成的列表:
```python
['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']
```

请注意，上面的简单tokenization方案主要用于将示例文本分离为单个单词，然而一些单词仍然与我们希望作为单独列表条目的标点符号相连接。我们也避免将所有文本都小写，因为大写有助于LLMs区分专有名词和普通名词、理解句子结构、学习并生成具有适当大写的文本。

让我们修改正则表达式在空格(\s)和逗号以及句号([，.])上的分割:
```python
result = re.split(r'([,.]|\s)', text)
print(result)
```

我们可以看到，单词和标点符号现在是独立的列表项，正如我们所希望的那样:
```python
['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']
```

剩下的一个小问题是该列表仍然包含空白字符。可选地，我们可以毫无顾忌地删除这些多余的字符，如下所示:
```python
result = [item for item in result if item.strip()]
print(result)
```

得到的无空白输出如下所示:
```python
['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']
```

> 是否删除空白
>
> 在开发一个简单的tokenizer时，我们是应该将空白编码为单独的字符还是直接删除它们取决于我们的应用程序及其需求。删除空白可以减少内存和计算需求。但是，如果我们训练对文本的确切结构敏感的模型(例如，Python代码对缩进和空格敏感)，保留空白可能是有用的。这里，为了使标记化的输出简单和简洁，我们删除了空白。稍后，我们将切换到包含空白的tokenization方案。

我们上面设计的tokenizer方案在简单的示例文本上工作得很好。让我们进一步修改它，以便它也可以处理其他类型的标点符号，例如问号、引号和我们在前面伊迪丝·华顿短篇故事的前100个字符中看到的双破折号，以及其他特殊字符:
```python
text = "Hello, world. Is this-- a test?"
result = re.split(r'([,.:;?_!"()\']|--|\s)', text)
result = [item.strip() for item in result if item.strip()]
print(result)
```

结果输出如下:
```python
['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']
```

根据图2.5总结的结果，我们可以看到，我们的tokenization方案现在可以成功地处理文本中的各种特殊字符。

![图2.5](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image009.png)
图2.5到目前为止，我们实现的tokenization方案将文本分割成单独的单词和标点符号。在此图所示的特定示例中，示例文本被分成10个单独的tokens。

现在我们有了一个基本的标记器，让我们把它应用到伊迪丝·沃顿的整个短篇故事中:
```python
preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', raw_text)
preprocessed = [item.strip() for item in preprocessed if item.strip()]
print(len(preprocessed))
```

上面的print语句输出4649，这是该文本中tokens的数量(不含空格)。

让我们打印前30个代币，以便快速进行人眼检查:
```python
print(preprocessed[:30])
```

结果输出显示，我们的tokenizer似乎很好地处理了文本，因为所有的单词和特殊字符都整齐地分开了:
```python
['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']
```

### 2.3 将tokens转换为tokens IDs
在前一节中，我们将Edith Wharton的一个短篇故事tokenize为单独的tokens。在本节中，我们将把这些tokens从Python字符串转换为整数表示，以生成所谓的tokens IDs。此转换是将tokens IDs转换为嵌入向量之前的中间步骤。

为了将之前生成的tokens映射到tokens IDs，我们必须首先构建一个所谓的词汇表。这个词汇表定义了如何将每个唯一的单词和特殊字符映射为唯一的整数，如图2.6所示。

![图 2.6](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image011.png)
图2.6 我们通过将训练数据集中的整个文本tokenize为单个tokens来构建词汇表。然后按字母顺序对这些单独的标记进行排序，并删除重复的标记。然后将唯一标记集合到一个词汇表中，该词汇表定义了从每个唯一标记到唯一整数值的映射。为了便于说明，所描述的词汇表故意设的很小，并且为了简单起见，不包含标点或特殊字符。

在上一节中，我们对Edith Wharton的短篇小说进行了tokenize，并将其赋值给一个名为preprocessed的Python变量。现在让我们创建一个包含所有唯一tokens的列表，并按字母顺序排序以确定字典大小:
```python
all_words = sorted(set(preprocessed))
vocab_size = len(all_words)
print(vocab_size)
```

通过上述代码确定词汇表大小为1,159之后，我们创建词汇表并打印其前50个条目以进行说明:

**<sub>清单2.2 创建词汇表<\sub>**
```python
vocab = {token:integer for integer,token in enumerate(all_words)}
for i, item in enumerate(vocab.items()):
    print(item)
    if i > 50:
        break
```

输出如下：
```python
('!', 0)
('"', 1)
("'", 2)
...
('Her', 49) 
('Hermia', 50) 
```

正如我们所看到的，基于上面的输出，字典包含与唯一整数标签相关联的单个tokens。我们的下一个目标是应用这个词汇表将新文本转换为tokens IDs，如图2.7所示。

![图 2.7](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image013.png)
图 2.7 从一个新的文本示例开始，我们对文本进行tokenize，并使用词汇表将文本tokens转换为token IDs。词汇表是从整个训练集中构建的，可以应用于训练集本身和任何新的文本样本。为简单起见，所描述的词汇表不包含标点符号或特殊字符。

在本书后面，当我们想要将LLM的输出从数字转换回文本时，我们还需要一种将token IDs转换为文本的方法。为此，我们可以创建词汇表的反向版本，将token IDs映射回相应的文本tokens。

让我们在Python中实现一个完整的tokenizer类，它使用encode方法将文本分割成tokens，并执行字符串到整数的映射，通过词汇表生成token IDs。此外，我们实现了一个decode方法，该方法执行整数到字符串的反向映射，将token IDs转换回文本。

这个tokenizer实现的代码如清单2.3所示:

**<sub>清单2.3 实现一个简单的文本tokenizer**
```python
class SimpleTokenizerV1:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = {i:s for s,i in vocab.items()}
    
    def encode(self, text):
        preprocessed = re.split(r'([,.?_!"()\']|--|\s)', text)
        preprocessed = [item.strip() for item in preprocessed if item.strip()]
        ids = [self.str_to_int[s] for s in preprocessed]
        return ids
        
    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids]) 
        
        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)
        return text
```

使用上面的SimpleTokenizerV1 Python类，我们现在可以通过现有词汇表实例化新的tokenizer对象，然后我们可以使用它来编码和解码文本，如图2.8所示。

![图2.8](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image015.png)
图2.8 Tokenizer的实现共享两个常用函数:编码函数和解码函数。encode函数接受示例文本，将其拆分为单个token，并通过词汇表将这些标记转换为token IDs。decode函数接受token IDs，将它们转换回文本tokens，并将文本tokens连接到自然文本中。

让我们从SimpleTokenizerV1类实例化一个新的tokenizer对象，并对Edith Wharton的短篇故事中的一段进行tokenize尝试:
```python
tokenizer = SimpleTokenizerV1(vocab)
text = """"It's the last he painted, you know," Mrs. Gisburn said with pardonable pride."""
ids = tokenizer.encode(text)
print(ids)
```

上面的代码输出以下token IDs:
```python
[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]
```

接下来，让我们看看是否可以使用decode方法将这些token IDs转换回文本:
```python
print(tokenizer.decode(ids))
```

输出如下文本:
```python
'" It\' s the last he painted, you know," Mrs. Gisburn said with pardonable pride.'
```

根据上面的输出，我们可以看到decode函数成功地将token IDs转换回原始文本。

到目前为止，一切顺利。我们实现了一个tokenizer，能够基于训练集的片段对文本进行tokenizing和tokenizing。现在让我们把它应用到一个不包含在训练集中的新文本样本上。现在让我们把它应用到一个不包含在训练集中的新文本样本上:
```python
text = "Hello, do you like tea?"
print(tokenizer.encode(text))
```

执行上述代码将导致以下错误:
```python
...
KeyError: 'Hello'
```

问题是在短篇小说《判决》中没有使用“你好”这个词。因此，它不包含在词汇表中。这突出表明了在LLMs上训练时需要考虑大型和多样化的训练集来扩展词汇表。

在下一节中，我们将在包含未知单词的文本上进一步测试tokenizer，并且我们还将讨论可用于在LLM训练期间为其提供进一步上下文连贯的其他特殊标记。

### 2.4 添加特殊的上下文tokens
在上一节中，我们实现了一个简单的tokenizer，并将其应用于训练集中的一个段落。在本节中，我们将修改这个tokenizer来处理未知单词。

我们还将讨论特殊上下文tokens的使用和添加，这些tokens可以增强模型对文本中上下文或其他相关信息的理解。例如，这些特殊的tokens可以包括未知单词和文档边界。

特别是，我们将修改在上一节SimpleTokenizerV2中实现的词汇表和tokenizer，以支持两个新的标记:<|unk|>和<|endoftext|>，如图2.9所示。

![图 2.9](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image017.png)
图 2.9 我们在词汇表中添加特殊的tokens来处理特定的上下文。例如，我们添加一个<|unk|> token来表示不属于训练数据的新单词和未知单词，它不属于现有词汇表。此外，我们添加了一个<|endoftext|>标记，可以用来分隔两个不相关的文本源。

如图2.9所示，我们可以修改tokenizer，使其在遇到不属于词汇表的单词时使用<|unk|>标记。此外，我们在不相关的文本之间添加token。例如，当在多个独立的文档或书籍上训练类GPT的LLMs时，通常在前面的文本源后面的每个文档或书籍之前插入一个token，如图2.10所示。这有助于LLMs理解，尽管这些文本源是为了训练而连接在一起的，但它们实际上是不相关的。

![图 2.10](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image019.png)
图 2.10 当使用多个独立文本源时，我们在这些文本之间添加<|endoftext|> tokens。这些<|endoftext|> tokens作为标记，指示特定段的开始或结束，允许LLM更有效地处理和理解。

现在让我们修改词汇表，将这两个特殊的tokens &lt;unk&gt;和<|endoftext|>添加到上一节中创建的所有唯一单词列表中，从而包含它们:
```python
all_tokens = sorted(list(set(preprocessed)))
all_tokens.extend(["<|endoftext|>", "<|unk|>"])
vocab = {token:integer for integer,token in enumerate(all_tokens)}
 
print(len(vocab.items()))
```

根据上面print语句的输出，新的词汇表大小为1161(上一节中的词汇表大小为1159)。

作为额外的快速检查，让我们打印最新词汇表的最后5个条目:
```python
for i, item in enumerate(list(vocab.items())[-5:]):
    print(item)
```

上面的代码输出如下:
```python
('younger', 1156)
('your', 1157)
('yourself', 1158)
('<|endoftext|>', 1159)
('<|unk|>', 1160)
```

根据上面的代码输出，我们可以确认这两个新的特殊标记确实成功地合并到了词汇表中。接下来，我们相应地调整清单2.3代码中的tokenizer，如清单2.4所示:

**<sub>清单2.4 处理未知单词的简单文本tokenizer**
```python
class SimpleTokenizerV2:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = { i:s for s,i in vocab.items()}
    
    def encode(self, text):
        preprocessed = re.split(r'([,.?_!"()\']|--|\s)', text)
        preprocessed = [item.strip() for item in preprocessed if item.strip()]
        preprocessed = [item if item in self.str_to_int
                        else "<|unk|>" for item in preprocessed]
 
        ids = [self.str_to_int[s] for s in preprocessed]
        return ids
        
    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])
 
        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)
        return text
```

与我们在前一节的代码清单2.3中实现的SimpleTokenizerV1相比，新的SimpleTokenizerV2用<|unk|>令牌替换未知单词。

现在让我们在实践中尝试这个新的标记器。为此，我们将使用一个简单的文本样本，我们将两个独立且不相关的句子连接起来:
```python
text1 = "Hello, do you like tea?"
text2 = "In the sunlit terraces of the palace."
text = " <|endoftext|> ".join((text1, text2))
print(text)
```

输出结果如下:
```python
'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'
```

接下来，让我们在清单2.2中创建的词汇表上使用SimpleTokenizerV2对示例文本进行tokenize:
```python
tokenizer = SimpleTokenizerV2(vocab)
print(tokenizer.encode(text))
```

打印以下token IDs:
```python
[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]
```

从上面我们可以看到token IDs列表包含用于<|endoftext|>分隔符tokens的1159以及用于未知单词的两个1160 tokens。

让我们对文本进行去de-tokenize以进行快速的完整性检查:
```python
print(tokenizer.decode(tokenizer.encode(text)))
```

输出结果如下:
```python
'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'
```

通过将上面de-tokenize的文本与原始输入文本进行比较，我们知道训练数据集伊迪丝·沃顿的短篇小说《判决》不包含单词“Hello”和“palace”。

到目前为止，我们已经讨论了将tokenization作为处理文本作为LLMs输入的重要步骤。根据LLMs的不同，一些研究人员还考虑了额外的特殊tokens，例如:
* [BOS](beginning of sequence，序列的开始):这个token标记文本的开始。对于LLMs来说，它表示一段内容从哪里开始。
* [EOS] (end of sequence，序列的结束):这个token位于文本的末尾，在连接多个不相关的文本时特别有用，类似于<|endoftext|>。例如，当组合两个不同的维基百科文章或书籍时，[EOS]令牌指示一篇文章的结束位置和下一篇文章的开始位置。
* [PAD](padding，填充):当训练批处理大小大于1的LLMs时，批处理可能包含不同长度的文本。为了确保所有文本具有相同的长度，使用[PAD]令牌对较短的文本进行扩展或“padding”直至批处理中最长文本的长度。

请注意，用于GPT模型的tokenizer不需要上面提到的任何标记，为了简单起见，它只使用<|endoftext|>标记。<|endoftext|>类似于上面提到的[EOS] token。此外，<|endoftext|>也用于padding。然而，正如我们将在后续章节中探讨的那样，当训练批处理输入时，我们通常使用一个mask（掩码），这意味着我们不需要关注被填充的tokens。因此，为padding选择的特定token变得无关紧要。

此外，用于GPT模型的tokenizer也不会对超出词汇表的单词使用<|unk|>标记。相反，GPT模型使用字节对编码tokenizer，它将单词分解为子词单元，我们将在下一节中讨论。

### 2.5 字节对编码
为了便于说明，我们在前面几节中实现了一个简单的tokenization方案。本节介绍一种更复杂的基于字节对编码(BPE)概念的tokenization方案。本节介绍的BPE tokenization被用于训练LLMs，如GPT-2、GPT-3和ChatGPT中使用的原始模型。

由于实现BPE可能相对复杂，我们将使用现有的Python开源库tiktoken (https://github.com/openai/tiktoken)，它基于Rust中的源代码非常有效地实现了BPE算法。与其他Python库类似，我们可以通过Python的pip安装程序从终端安装tiktoken库:
```python
pip install tiktoken
```

本章中的代码基于tiktoken 0.5.1。您可以使用以下代码来检查当前安装的版本:
```python
from importlib.metadata import version
import tiktoken
print("tiktoken version:", version("tiktoken"))
```

安装完成后，我们可以从tiktoken实例化BPE tokenizer，如下所示:
```python
tokenizer = tiktoken.get_encoding("gpt2")
```

这个tokenizer的用法类似于我们之前通过encode函数实现的SimpleTokenizerV2:
```python
text = "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace."
integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})
print(integers)
```

上面的代码输出以下token IDs:
```python
[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]
```

然后我们可以使用decode方法将token IDs转换回文本，类似于前面的SimpleTokenizerV2:
```python
strings = tokenizer.decode(integers)
print(strings)
```

上面的代码输出如下:

```python
'Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.'
```

基于上面的token IDs和解码文本，我们可以得出两个值得注意的观察结果。首先，为<|endoftext|> token分配一个相对较大的令牌ID，即50256。实际上，用于训练GPT-2、GPT-3和ChatGPT中使用的原始模型等模型的BPE tokenizer的总词汇量为50,257，其中<|endoftext|>被分配为最大的token ID。

其次，上面的BPE tokenizer可以正确地编码和解码未知单词，例如“someunknownPlace”。BPE tokenizer可以处理任何未知的单词。但它如何在不使用<|unk|>令牌的情况下实现这一点的呢?

BPE的底层算法将不在其预先定义的词汇表中的单词分解为更小的子词单位甚至单个字符，从而使其能够处理词汇表外的单词。因此，由于BPE算法，如果tokenizer在tokenization过程中遇到不熟悉的单词，它可以将其表示为子词标记或字符序列，如图2.11所示。

![图2.11](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image021.png)
图2.11 BPE tokenizers将未知单词分解为子单词和单个字符。通过这种方式，BPE tokenizers可以解析任何单词，而不需要用特殊标记(如<|unk|>)替换未知单词。

如图2.11所示，将未知单词分解为单个字符的能力确保了tokenizer以及由此训练的LLMs可以处理任何文本，即使它包含训练数据中不存在的单词。

> 练习2.1:未知单词的字节对编码
>
> 尝试使用tiktoken库中的BPE tokenizer对未知单词“Akwirw ier”进行tokenization，并打印单个token IDs。然后，对该列表中的每个结果整数调用decode函数，以重现图2.11所示的映射。最后，调用token IDs上的decode方法，检查它是否可以重建原始输入“Akwirw ier”。

BPE的详细讨论和实现超出了本书的范围，但简而言之，它通过迭代地将频繁字符合并到子词中并将频繁子词合并到词中来构建其词汇表。例如，BPE首先将所有单个字符添加到其词汇表中(“a”，“b”，…)。在下一阶段，它将经常出现在一起的字符组合合并成子词。例如，“d”和“e”可能合并成副词“de”，这在许多英语单词中很常见，如“define”、“depend”、“made”和“hidden”。是否合并由截断频率决定。

### 2.6 使用滑动窗口进行数据采样
前一节非常详细地介绍了tokenization步骤和从字符串tokens到整数token IDs的转换。在我们最终为LLM创建嵌入之前的下一步是生成训练LLM所需的输入-目标对。

这些输入-目标对是什么样的?正如我们在第1章中所学到的，LLMs通过预测文本中的下一个单词来进行预训练，如图2.12所示。

![图 2.12](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image023.png)
图 2.12 给定一个文本样本，提取输入块作为子样本作为LLM的输入，LLM在训练时的预测任务是预测输入块后面的下一个单词。在训练过程中，我们会屏蔽掉所有超出目标的单词。请注意，在LLM处理此图中显示的文本之前，将对其进行tokenization;但是，为了清晰起见，此图省略了tokenization步骤。

在本节中，我们实现一个数据加载器，它使用滑动窗口方法从训练数据集中获取图2.12所示的输入-目标对。

在开始之前，我们将首先使用上一节介绍的BPE tokenizer对之前处理过的整个the Verdict短篇故事进行tokenize：
```python
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
 
enc_text = tokenizer.encode(raw_text)
print(len(enc_text))    
```

在应用BPE tokenizer后，执行上面的代码将返回5145，即训练集中的标记总数。

接下来，出于演示目的，我们从数据集中删除前50个tokens，因为它会在接下来的步骤中产生一个稍微有趣的文本段落:
```python
enc_sample = enc_text[50:]  
```
    
为下一个单词预测任务创建输入-目标对的最简单和最直观的方法之一是创建两个变量x和y，其中x包含输入tokens，y包含目标，即移位1的输入:
```python
context_size = 4
x = enc_sample[:context_size]
y = enc_sample[1:context_size+1]
print(f"x: {x}")
print(f"y:      {y}") 
```

运行上面的代码输出如下:
```python
x: [290, 4920, 2241, 287]
y:      [4920, 2241, 287, 257]
```

将输入与目标一起处理，目标是移动了一个位置的输入，然后我们可以创建图2.12中所示的下一个单词预测的方案，如下所示:
```python
for i in range(1, context_size+1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(context, "---->", desired)
```

上面的代码输出如下内容:
```python
[290] ----> 4920
[290, 4920] ----> 2241
[290, 4920, 2241] ----> 287
[290, 4920, 2241, 287] ----> 257
```

箭头左边的所有内容(---->)都是指LLM将接收的输入，箭头右边的token ID表示LLM应该预测的目标token ID。

为了便于说明，让我们重复前面的代码，但将tokrn ID转换为文本:
```python
for i in range(1, context_size+1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(tokenizer.decode(context), "---->", tokenizer.decode([desired]))
```



### 2.7 创建token嵌入

### 2.8 对单词位置进行编码

### 2.9 小结
---
layout: post
title: 《从头开始构建大语言模型》02
date: 2024-07-14
author: 曾伟
tags: [论文阅读]
comments: true
toc: true # 目录
# pinned: true 是否置顶
---

简介：紧接[上一章](https://zengweithu.github.io/ZengWei-Home/LLMBook1/)，这是《从头开始构建大语言模型》的第2章。紧急消息：原来在[MANNING上的Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)上分章阅读当我读到2.1的时候就不给我读了，提示我需要收费，好吧，非常sad。看来不得不掏出我自己找到的老版[PDF](https://github.com/ZengWeiTHU/eBook/blob/main/LLM/Build-a-Large-Language-Model-From-Scratch.pdf)了。另外，我还发现[SCRIBD](https://www.scribd.com/home)也只有前30天免费，果然这是一个处处要钱的世界。

## 2 使用文本数据
> 本章涵盖：
> * 为大型语言模型训练准备文本
> * 将文本拆分为单词和子单词tokens
> * tokenizing文本更高级的字节对编码
> * 使用滑动窗口方法的抽样训练样本
> * 将tokens转换为输送到大型语言模型的向量

在上一章中，我们介绍了大型语言模型（LLMs）的通用结构，并了解到它们是在大量文本数据上进行预先训练的。具体来说，我们的重点是在基于Transformer架构的LLM中的解码器，该架构是 ChatGPT 和其他流行的类似 LLMsGPT 的模型的基础。

在预训练阶段，LLMs一次处理一个单词文本。使用下一个单词预测任务对数百万到数十亿个参数进行LLMs训练，可以产生具有令人印象深刻功能的模型。然后，可以进一步微调这些模型，以遵执行通用命令或特定的目标任务。但是，在接下来的章节中实现和训练LLMs之前，我们需要准备训练数据集，这是本章的重点，如图 2.1 所示。

![图 2.1](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image001.png)
图 2.1 编程实现一个LLM有三个阶段，其中包括LLM在通用文本数据集上进行预训练，并在标记数据集上对其进行微调。本章将解释和编程实现数据的准备和采样流程，该流程提供用于LLM预训练的文本数据。

在本章中，您将学习如何准备用于训练LLMs的输入文本。这涉及将文本拆分为单独的单词和子单词tokens，然后可以将其编码为LLM需要的向量表示。您还将了解高级tokenization方案，例如字节对编码，该方案用于GPT 等流行LLMs。最后，我们将实现采样和数据加载策略，以生成后续章节中训练LLMs所需的输入-输出对。

### 2.1 理解词嵌入
深度神经网络模型（包括 LLMs）无法直接处理原始文本。由于文本是离散的，因此它与用于实现和训练神经网络的数学运算不兼容。因此，我们需要一种方法来将单词表示为连续值向量。（不熟悉计算中的向量和张量的读者可以在附录 A 的 A2.2 节“理解张量”中了解更多信息。）

将数据转换为向量格式这一概念通常称为嵌入（embedding）。使用特定的神经网络层或其他预训练的神经网络模型，我们可以嵌入不同的数据类型，例如视频、音频和文本，如图 2.2 所示。

![图 2.2](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image003.png)
图 2.2 深度学习模型无法以原始形式处理视频、音频和文本等数据格式。因此，我们使用嵌入模型将这些原始数据转换为深度学习架构可以轻松理解和处理的稠密向量表示。具体来说，该图说明了将原始数据转换为三维数值向量的过程。

如图2.2所示，我们可以通过嵌入模型来处理各种不同的数据格式。但是，需要注意的是，不同的数据格式需要不同的嵌入模型。例如，为文本设计的嵌入模型不适合嵌入音频或视频数据。

嵌入的核心是从离散对象(如单词、图像甚至整个文档)到连续向量空间中的点的映射——嵌入的主要目的是将非数字数据转换为神经网络可以处理的数据格式。

虽然词嵌入是最常见的文本嵌入形式，但也有句子、段落或整个文档的嵌入。句子或段落嵌入是检索增强式生成（retrievalaugmented generation）的常用选择。检索式增强生成将生成(如生成文本)和检索(如搜索外部知识库)结合起来，在生成文本时提取相关信息，这是一种超出本书范围的技术。由于我们的目标是训练类似GPT的LLMs，它一次生成一个词，因此本章主要关注词嵌入。

目前已经开发了几种算法和框架来生成词嵌入，一个较早且最流行的例子是Word2Vec方法。Word2Vec训练神经网络架构，通过预测给定目标词的词的上下文来生成词嵌入，反之亦然。Word2Vec背后的主要思想是：出现在相似上下文中的单词往往具有相似的含义。因此，为了可视化的目的，当投影到二维词嵌入中时，可以看到相似的术语单词聚集在一起，如图2.3所示。

![图 2.3 ](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image005.png)
图 2.3 如果词嵌入是二维的，我们可以将它们绘制在二维散点图中，以便可视化，如下所示。当使用词嵌入技术（如Word2Vec）时，将相似概念相对应的词在嵌入空间中经常彼此靠近。例如，与国家和城市相比，不同类型的鸟类在嵌入空间中看起来彼此更接近。

单词嵌入可以有不同的维度，从1到数千。如图2.3所示，为了可视化目的，我们可以选择二维词嵌入。更高的维度可能捕获更细微的关系，但代价是计算效率。

虽然我们可以使用预训练模型(如Word2Vec)为机器学习模型生成嵌入，但LLMs通常会生成自己的嵌入，这些嵌入是输入层的一部分，并在训练期间更新。作为LLMs训练的一部分而不是使用Word2Vec来进行词嵌入的优点是，该方式针对当前的特定任务的数据进行了优化。我们将在本章后面实现这样的嵌入层。此外，LLMs还可以创建上下文相关的输出词嵌入，我们将在第3章中讨论。

不幸的是，高维嵌入给可视化带来了挑战，因为我们的感官知觉和常见的图形表示本质上局限于三维或更少的维度，这就是为什么图2.3在二维散点图中显示二维嵌入。然而，当使用LLMs时，我们通常使用比图2.3所示的维度高得多的词嵌入。对于GPT-2和GPT-3，嵌入大小(通常称为模型隐藏状态的维数)根据特定的模型变量和大小而变化。这是性能和效率之间的权衡。最小的GPT-2模型(117M和125M参数)使用768维的嵌入尺寸作为具体的例子。最大的GPT-3模型(175B参数)则使用12288个维度的嵌入大小。

本章接下来的章节将介绍准备LLM使用的嵌入所需的步骤，包括将文本分解为单词，将单词转换为tokens，并将tokens转换为嵌入向量。

### 2.2 Tokenizing文本
本节介绍如何将输入文本拆分为单个tokens，这是为LLM创建嵌入所需的必要预处理步骤。这些符号可以是单独的单词或特殊字符，包括标点符号，如图2.4所示。

![](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image007.png)
图2.4 本节中涉及在LLM上下文中的进行文本处理的可视化流程图。这里，我们将输入文本拆分为单独的tokens，这些tokens可以是单词，也可以是特殊字符，如标点符号。在接下来的部分中，我们将把文本转换为tokens IDs并创建tokens嵌入。

我们将用于LLM训练tokens的文本是伊迪丝·沃顿(Edith Wharton)的一篇短篇小说《判决》(The Verdict)，它是公开的，因此被允许用于LLMs训练任务。该文本可在Wikisource：https://en.wikisource.org/wiki/The_Verdict 上获得，您可以将其复制并粘贴到文本文件中，我将其复制到文本文件"the-verdict.txt"中，以便使用Python的标准文件读取实用程序加载:

**<sub>清单2.1将一个短篇故事作为文本样本读入Python</sub>**
```python
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
print("Total number of character:", len(raw_text))
print(raw_text[:99])
```

或者，您可以在本书的GitHub存储库 https://github.com/rasbt/LLMs-fromscratch/tree/main/ch02/01_main-chapter-code 中找到“the-verdict.txt”文件。

为了便于说明，print命令打印该文件的前100个字符后的总字符数:
```python
Total number of character: 20479
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no 
```

我们的目标是将这20,479个字符的短篇故事tokenize为单个单词和特殊字符，然后我们可以在接下来的章节中将其转化为LLMs训练的次嵌入。

> 文本样本的大小
>
> 请注意，在训练LLMs时，通常要处理数百万篇文章和数十万本书(许多GB的文本)。但是，出于教育目的，使用较小的文本示例(例如一本书)来说明文本处理步骤背后的主要思想并使其能够在合理的时间内在消费者硬件上运行就足够了。

我们如何分割文本以获得tokens列表?为此，我们进行了一个小的尝试，并使用Python的正则表达式库re进行说明。(注意，你不需要学习或记忆任何正则表达式语法，因为我们将在本章后面会过渡到提前构建的tokenizer函数。)

使用一些简单的示例文本，我们可以使用re.split命令和以下语法来分割空白字符的文本:
```python
import re
text = "Hello, world. This, is a test."
result = re.split(r'(\s)', text)
print(result)
```

结果是一个由单个单词、空格和标点字符组成的列表:
```python
['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']
```

请注意，上面的简单tokenization方案主要用于将示例文本分离为单个单词，然而一些单词仍然与我们希望作为单独列表条目的标点符号相连接。我们也避免将所有文本都小写，因为大写有助于LLMs区分专有名词和普通名词、理解句子结构、学习并生成具有适当大写的文本。

让我们修改正则表达式在空格(\s)和逗号以及句号([，.])上的分割:
```python
result = re.split(r'([,.]|\s)', text)
print(result)
```

我们可以看到，单词和标点符号现在是独立的列表项，正如我们所希望的那样:
```python
['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']
```

剩下的一个小问题是该列表仍然包含空白字符。可选地，我们可以毫无顾忌地删除这些多余的字符，如下所示:
```python
result = [item for item in result if item.strip()]
print(result)
```

得到的无空白输出如下所示:
```python
['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']
```

> 是否删除空白
>
> 在开发一个简单的tokenizer时，我们是应该将空白编码为单独的字符还是直接删除它们取决于我们的应用程序及其需求。删除空白可以减少内存和计算需求。但是，如果我们训练对文本的确切结构敏感的模型(例如，Python代码对缩进和空格敏感)，保留空白可能是有用的。这里，为了使标记化的输出简单和简洁，我们删除了空白。稍后，我们将切换到包含空白的tokenization方案。

我们上面设计的tokenizer方案在简单的示例文本上工作得很好。让我们进一步修改它，以便它也可以处理其他类型的标点符号，例如问号、引号和我们在前面伊迪丝·华顿短篇故事的前100个字符中看到的双破折号，以及其他特殊字符:
```python
text = "Hello, world. Is this-- a test?"
result = re.split(r'([,.:;?_!"()\']|--|\s)', text)
result = [item.strip() for item in result if item.strip()]
print(result)
```

结果输出如下:
```python
['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']
```

根据图2.5总结的结果，我们可以看到，我们的tokenization方案现在可以成功地处理文本中的各种特殊字符。

![图2.5](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image009.png)
图2.5到目前为止，我们实现的tokenization方案将文本分割成单独的单词和标点符号。在此图所示的特定示例中，示例文本被分成10个单独的tokens。

现在我们有了一个基本的标记器，让我们把它应用到伊迪丝·沃顿的整个短篇故事中:
```python
preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', raw_text)
preprocessed = [item.strip() for item in preprocessed if item.strip()]
print(len(preprocessed))
```

上面的print语句输出4649，这是该文本中tokens的数量(不含空格)。

让我们打印前30个代币，以便快速进行人眼检查:
```python
print(preprocessed[:30])
```

结果输出显示，我们的tokenizer似乎很好地处理了文本，因为所有的单词和特殊字符都整齐地分开了:
```python
['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']
```

### 2.3 将tokens转换为tokens IDs
在前一节中，我们将Edith Wharton的一个短篇故事tokenize为单独的tokens。在本节中，我们将把这些tokens从Python字符串转换为整数表示，以生成所谓的tokens IDs。此转换是将tokens IDs转换为嵌入向量之前的中间步骤。

为了将之前生成的tokens映射到tokens IDs，我们必须首先构建一个所谓的词汇表。这个词汇表定义了如何将每个唯一的单词和特殊字符映射为唯一的整数，如图2.6所示。

![图 2.6](https://drek4537l1klr.cloudfront.net/raschka/v-8/Figures/ch02__image011.png)
图2.6 我们通过将训练数据集中的整个文本tokenize为单个tokens来构建词汇表。然后按字母顺序对这些单独的标记进行排序，并删除重复的标记。然后将唯一标记集合到一个词汇表中，该词汇表定义了从每个唯一标记到唯一整数值的映射。为了便于说明，所描述的词汇表故意设的很小，并且为了简单起见，不包含标点或特殊字符。

在上一节中，我们对Edith Wharton的短篇小说进行了tokenize，并将其赋值给一个名为preprocessed的Python变量。现在让我们创建一个包含所有唯一tokens的列表，并按字母顺序排序以确定字典大小:
```python
all_words = sorted(set(preprocessed))
vocab_size = len(all_words)
print(vocab_size)
```

通过上述代码确定词汇表大小为1,159之后，我们创建词汇表并打印其前50个条目以进行说明:

**<sub>清单2.2 创建词汇表<\sub>**
```python
vocab = {token:integer for integer,token in enumerate(all_words)}
for i, item in enumerate(vocab.items()):
    print(item)
    if i > 50:
        break
```

输出如下：
```python
('!', 0)
('"', 1)
("'", 2)
...
('Her', 49) 
('Hermia', 50) 
```

正如我们所看到的，基于上面的输出，字典包含与唯一整数标签相关联的单个tokens。我们的下一个目标是应用这个词汇表将新文本转换为tokens IDs，如图2.7所示。

![]()


### 2.4 添加特殊的上下文tokens

### 2.5 字节对编码

### 2.6 使用滑动窗口进行数据采样

### 2.7 创建token嵌入

### 2.8 对单词位置进行编码

### 2.9 小结